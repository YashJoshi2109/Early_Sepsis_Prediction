{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2844852,
          "sourceType": "datasetVersion",
          "datasetId": 1740766
        },
        {
          "sourceId": 4825347,
          "sourceType": "datasetVersion",
          "datasetId": 2795407
        }
      ],
      "dockerImageVersionId": 30357,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashJoshi2109/Early_Sepsis_Prediction/blob/main/Early_sepsis_prediction_Synthetic_Dataset_(6hours_early).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all the required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, mean_absolute_error, mean_squared_error\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "-ZNCUyu9Z2rf",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and splitting the data into training and testing"
      ],
      "metadata": {
        "id": "v31oZ0sF4p4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0a44rYHBhmWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined = pd.read_csv('/content/drive/MyDrive/Dataset.csv')"
      ],
      "metadata": {
        "id": "YFMuD2HVhe4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patients = list(combined['Patient_ID'].unique())\n",
        "len(patients)"
      ],
      "metadata": {
        "id": "_9pM_8hKMk7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the file with the combined data of both the hospitals and splitting it into 2 parts\n",
        "\n",
        "#  = pd.read_csv('/kaggle/input/prediction-of-sepsis/Dataset.csv')\n",
        "#sepsis-physionet\n",
        "\n",
        "rows_to_drop = combined.loc[combined['Patient_ID'].apply(lambda x: len(str(x)) == 6)]\n",
        "df_train = combined.drop(rows_to_drop.index)\n",
        "df_train.to_csv('data_part1.csv', index=False)\n",
        "\n",
        "rows_to_drop = combined.loc[combined['Patient_ID'].apply(lambda x: len(str(x)) != 6)]\n",
        "df_test = combined.drop(rows_to_drop.index)\n",
        "df_test.to_csv('data_part2.csv', index=False)"
      ],
      "metadata": {
        "id": "HD6N8v4Z3lFL",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:23:28.433933Z",
          "iopub.execute_input": "2024-06-15T14:23:28.434345Z",
          "iopub.status.idle": "2024-06-15T14:24:02.372294Z",
          "shell.execute_reply.started": "2024-06-15T14:23:28.43431Z",
          "shell.execute_reply": "2024-06-15T14:24:02.371156Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the test and training data\n",
        "\n",
        "df_train = pd.read_csv('/content/data_part1.csv')\n",
        "df_test = pd.read_csv('/content/data_part2.csv')"
      ],
      "metadata": {
        "id": "sgOnu_TkZ2rh",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:24:11.342349Z",
          "iopub.execute_input": "2024-06-15T14:24:11.342774Z",
          "iopub.status.idle": "2024-06-15T14:24:15.280252Z",
          "shell.execute_reply.started": "2024-06-15T14:24:11.342725Z",
          "shell.execute_reply": "2024-06-15T14:24:15.279228Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data first five columns\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "T4mhbcHTeS0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data first five columns\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "6LAWebENeVCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory data analysis"
      ],
      "metadata": {
        "id": "NLDcqUUC45Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(15)"
      ],
      "metadata": {
        "id": "Whd_uDuE5Foz",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:24:28.621152Z",
          "iopub.execute_input": "2024-06-15T14:24:28.621585Z",
          "iopub.status.idle": "2024-06-15T14:24:28.663385Z",
          "shell.execute_reply.started": "2024-06-15T14:24:28.621549Z",
          "shell.execute_reply": "2024-06-15T14:24:28.66206Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, this is a time series data. The dataframe has 44 columns. After the first loook at data, it can be observed that the data seems quite sparse."
      ],
      "metadata": {
        "id": "Ml0Er11g5Ofz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's list down all the different columns\n",
        "\n",
        "df_train.columns"
      ],
      "metadata": {
        "id": "bfpcLoUNZ2ri",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:24:36.512331Z",
          "iopub.execute_input": "2024-06-15T14:24:36.512715Z",
          "iopub.status.idle": "2024-06-15T14:24:36.520642Z",
          "shell.execute_reply.started": "2024-06-15T14:24:36.512683Z",
          "shell.execute_reply": "2024-06-15T14:24:36.519421Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# list of all the unique patients\n",
        "# total number of patients present in the dataset\n",
        "\n",
        "patients = list(df_test['Patient_ID'].unique())\n",
        "len(patients)"
      ],
      "metadata": {
        "id": "kIAeEgYGZ2rl",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:24:48.129732Z",
          "iopub.execute_input": "2024-06-15T14:24:48.130581Z",
          "iopub.status.idle": "2024-06-15T14:24:48.149049Z",
          "shell.execute_reply.started": "2024-06-15T14:24:48.130539Z",
          "shell.execute_reply": "2024-06-15T14:24:48.147853Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to draw correlation heat map\n",
        "\n",
        "def corr_matrix(df):\n",
        "  corr = df.corr()\n",
        "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "  f, ax = plt.subplots(figsize=(40,40))\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "  sns.heatmap(corr, mask=mask, cmap=\"Paired\", vmax=.3, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "\n"
      ],
      "metadata": {
        "id": "pdpbh5SSBU2y",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:24:53.820279Z",
          "iopub.execute_input": "2024-06-15T14:24:53.821221Z",
          "iopub.status.idle": "2024-06-15T14:24:53.828148Z",
          "shell.execute_reply.started": "2024-06-15T14:24:53.821175Z",
          "shell.execute_reply": "2024-06-15T14:24:53.826844Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of null values present in all the parameters\n",
        "\n",
        "null_values = df_train.isnull().mean()*100\n",
        "null_values = null_values.sort_values(ascending=False)\n",
        "null_values"
      ],
      "metadata": {
        "id": "ydcFf7jS-Hkm",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:25:02.352438Z",
          "iopub.execute_input": "2024-06-15T14:25:02.352851Z",
          "iopub.status.idle": "2024-06-15T14:25:02.430275Z",
          "shell.execute_reply.started": "2024-06-15T14:25:02.352819Z",
          "shell.execute_reply": "2024-06-15T14:25:02.429216Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image obtained from the official pdf of the PhysioNet Challenge. The image gives an overall idea of the number of records available for each feature, for all the three datasets of three different hospitals."
      ],
      "metadata": {
        "id": "rx1tX9VAEDbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After thorough analysis of the data, it was found that a lot of the features were redundant, i.e. a lot of the features were indicative of the same parameter, so those features were removed on the basis of their sparsity. Another criteria taken into consideration was ease of getting the parameter among different hospitals, becuase some of the parameters are easier to obtain for a given hospital while some are expensive and for some a hospital does not even have infrastructure to get those features."
      ],
      "metadata": {
        "id": "UsUJypES9JVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the features have been removed based on the number of null values and redundancy\n",
        "\n",
        "# 'Unnamed: 0'-- this is the index column\n",
        "# 'SBP' -- MAP is considered instead of SBP and DBP\n",
        "# 'DBP' -- MAP is considered instead of SBP and DBP\n",
        "# 'EtCO2' -- It is not readily. available and also not available in the dataset\n",
        "# 'BaseExcess' -- It is an indicator of the health of kidney and pancreas but is redundant here becuase a lot of different features for kidney are being considered here.\n",
        "# 'HCO3' -- It is also an indicator of the health of kidney and pancreas but is redundant here becuase a lot of different features for kidney are being considered here.\n",
        "# 'pH' -- It is also an indicator of the health of kidney and pancreas but is redundant here becuase a lot of different features for kidney are being considered here.\n",
        "# 'PaCO2' -- It is an indicator of respiratory health of an individual, we are considering SaO2 and FiO2 which are more readily avaiilable\n",
        "# 'Alkalinephos' -- It is an indicator of the health of the liver of an individual, but we are considering Bilirubin instead of this, as data of this is more scarce.\n",
        "# 'Calcium' -- It is also an indicator of the health of kidney but is redundant here becuase a lot of different features for kidney are being considered here.\n",
        "# 'Magnesium' -- It is an indicator of metabolism and kidney health, but we are considering Lactate instead as it is more readily available\n",
        "# 'Phosphate' -- It is also an indicator of the health of kidney but is redundant here becuase a lot of different features for kidney are being considered here.\n",
        "# 'Potassium' -- It is also an indicator of the health of kidney but is redundant here becuase a lot of different features for kidney are being considered here.\n",
        "# 'PTT' -- It is used to check inflammation. We are using WBC counts instead as it is easy to obtain.\n",
        "# 'Fibrinogen' -- It is used to check blood clotting ability. We are using Platelets instead, as it is easier to obtain.\n",
        "# 'Unit1' -- Both the columns Unit1 and Unit2 have been merged to form a single column.\n",
        "# 'Unit2' -- Both the columns Unit1 and Unit2 have been merged to form a single column.\n",
        "\n",
        "columns_drop={'Unnamed: 0','SBP','DBP','EtCO2','BaseExcess', 'HCO3','pH','PaCO2','Alkalinephos', 'Calcium','Magnesium',\n",
        "'Phosphate','Potassium','PTT','Fibrinogen','Unit1','Unit2'}\n",
        "df_train = df_train.assign(Unit=df_train['Unit1'] + df_train['Unit2'])\n",
        "df_train_mod = df_train.drop(columns=columns_drop)\n",
        "df_train_mod.columns"
      ],
      "metadata": {
        "id": "0wbArkKfZ2rm",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:25:21.376278Z",
          "iopub.execute_input": "2024-06-15T14:25:21.376691Z",
          "iopub.status.idle": "2024-06-15T14:25:21.718917Z",
          "shell.execute_reply.started": "2024-06-15T14:25:21.376657Z",
          "shell.execute_reply": "2024-06-15T14:25:21.717789Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation matrix to check if there is high correlation between the remaining features\n",
        "\n",
        "corr_matrix(df_train_mod)"
      ],
      "metadata": {
        "id": "nQKphFO8Z2rn",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:25:31.488953Z",
          "iopub.execute_input": "2024-06-15T14:25:31.48937Z",
          "iopub.status.idle": "2024-06-15T14:25:33.675816Z",
          "shell.execute_reply.started": "2024-06-15T14:25:31.489334Z",
          "shell.execute_reply": "2024-06-15T14:25:33.67446Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen in this correlation heat map almost all of the feature do not have high correlation."
      ],
      "metadata": {
        "id": "mDoY1QwZZ2ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation"
      ],
      "metadata": {
        "id": "I0lJ7KEgZ2rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are a lot of missing values in the dataset, therefore imputation was done to fill the missing values. While imputing, it is important to note that ***imputation should be done on per patient basis,*** otherwise the data from one patient will leak into the data of the other patient. Also another point that should be taken into consideration is that mean, median, mode can not directly be used to impute as it will result in uneven distribution of the parameters with respect to time."
      ],
      "metadata": {
        "id": "Wggbo0LaLeeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_impute = df_train_mod.copy()\n",
        "columns_impute = list(df_train_impute.columns)"
      ],
      "metadata": {
        "id": "Vr5NQe2oZ2rt",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:25:41.998892Z",
          "iopub.execute_input": "2024-06-15T14:25:41.999698Z",
          "iopub.status.idle": "2024-06-15T14:25:42.066466Z",
          "shell.execute_reply.started": "2024-06-15T14:25:41.999659Z",
          "shell.execute_reply": "2024-06-15T14:25:42.065235Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# bfill and ffill for imputing\n",
        "\n",
        "grouped_by_patient = df_train_impute.groupby('Patient_ID')\n",
        "df_train_impute = grouped_by_patient.apply(lambda x: x.bfill().ffill())"
      ],
      "metadata": {
        "id": "qCscSc6AnLTW",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:25:53.113884Z",
          "iopub.execute_input": "2024-06-15T14:25:53.114296Z",
          "iopub.status.idle": "2024-06-15T14:26:09.670794Z",
          "shell.execute_reply.started": "2024-06-15T14:25:53.11426Z",
          "shell.execute_reply": "2024-06-15T14:26:09.669456Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_impute.head()"
      ],
      "metadata": {
        "id": "VwfwspVIrwn6",
        "execution": {
          "iopub.status.busy": "2024-06-15T13:08:50.066215Z",
          "iopub.execute_input": "2024-06-15T13:08:50.06674Z",
          "iopub.status.idle": "2024-06-15T13:08:50.098432Z",
          "shell.execute_reply.started": "2024-06-15T13:08:50.066692Z",
          "shell.execute_reply": "2024-06-15T13:08:50.09729Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's check the remaining proportion of missing values\n",
        "\n",
        "null_values = df_train_impute.isnull().mean()*100\n",
        "null_values = null_values.sort_values(ascending=False)\n",
        "null_values"
      ],
      "metadata": {
        "id": "680T86e2nyvF",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:26:09.673135Z",
          "iopub.execute_input": "2024-06-15T14:26:09.673494Z",
          "iopub.status.idle": "2024-06-15T14:26:09.724776Z",
          "shell.execute_reply.started": "2024-06-15T14:26:09.67346Z",
          "shell.execute_reply": "2024-06-15T14:26:09.723563Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "'TroponinI', 'Bilirubin_direct', 'AST', 'Bilirubin_total', 'Lactate', 'SaO2', 'FiO2', 'Unit', 'Patient_ID'  have more than 25 percent of null values and hence are dropped from the dataset."
      ],
      "metadata": {
        "id": "Yuep2B2ZM_u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping all the columns with null values more than 25% and patient_id\n",
        "\n",
        "null_col = ['TroponinI', 'Bilirubin_direct', 'AST', 'Bilirubin_total', 'Lactate', 'SaO2', 'FiO2',\n",
        "            'Unit', 'Patient_ID']\n",
        "df_train_impute = df_train_impute.drop(columns=null_col)\n",
        "df_train_impute.columns"
      ],
      "metadata": {
        "id": "UQFsROyVxBnt",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:26:17.48218Z",
          "iopub.execute_input": "2024-06-15T14:26:17.4826Z",
          "iopub.status.idle": "2024-06-15T14:26:17.532721Z",
          "shell.execute_reply.started": "2024-06-15T14:26:17.482563Z",
          "shell.execute_reply": "2024-06-15T14:26:17.531308Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's check the remaining proportion of missing values\n",
        "\n",
        "null_values = df_train_impute.isnull().mean()*100\n",
        "null_values = null_values.sort_values(ascending=False)\n",
        "null_values"
      ],
      "metadata": {
        "id": "rVeSlMdbfKHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# onehot encoding the gender\n",
        "\n",
        "one_hot = pd.get_dummies(df_train_impute['Gender'])\n",
        "df_train_impute = df_train_impute.join(one_hot)\n",
        "df_train_impute = df_train_impute.drop('Gender', axis=1)\n"
      ],
      "metadata": {
        "id": "RcYNzPLrQjP7",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:26:24.33229Z",
          "iopub.execute_input": "2024-06-15T14:26:24.332701Z",
          "iopub.status.idle": "2024-06-15T14:26:24.521071Z",
          "shell.execute_reply.started": "2024-06-15T14:26:24.332666Z",
          "shell.execute_reply": "2024-06-15T14:26:24.519968Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Transformation and Standard Normalization"
      ],
      "metadata": {
        "id": "MlNnOUoNvaCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally models tend to give a better result for a normal ditribution. So in the below cells we explored different techniques to plot histograms and QQ plots of all the features and then we applied different transformations on it to see which were giving good results. The ones giving the beest results were then adopted in the dataframe."
      ],
      "metadata": {
        "id": "c1mAw9UAps1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to draw histogram and QQ plot\n",
        "\n",
        "def diagnostic_plots(df, variable):\n",
        "    fig = plt.figure(figsize=(15,4))\n",
        "    ax = fig.add_subplot(121)\n",
        "    df[variable].hist(bins=30)\n",
        "    ax = fig.add_subplot(122)\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
        "    plt.xlabel(variable)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sd9zEOxmZaak",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:26:38.577616Z",
          "iopub.execute_input": "2024-06-15T14:26:38.578018Z",
          "iopub.status.idle": "2024-06-15T14:26:38.585146Z",
          "shell.execute_reply.started": "2024-06-15T14:26:38.577985Z",
          "shell.execute_reply": "2024-06-15T14:26:38.583842Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def try_gaussian(df, col):\n",
        "  print('actual plot')\n",
        "  diagnostic_plots(df,col)\n",
        "  # this applies yeojohnson plot\n",
        "  df['col_yj'], param = stats.yeojohnson(df[col])\n",
        "  print('yeojohnson plot')\n",
        "  diagnostic_plots(df, 'col_yj')\n",
        "  # this applies exponential transformation\n",
        "  df['col_1.5'] = df[col]**(1/1.5)\n",
        "  print('**1/1.5 plot')\n",
        "  diagnostic_plots(df, 'col_1.5')\n",
        "  df['col_.5'] = df[col]**(.5)\n",
        "  print('**.5 plot')\n",
        "  # this applies inverse transformation\n",
        "  diagnostic_plots(df, 'col_.5')\n",
        "  df['col_rec'] = 1 / (df[col]+0.00001)\n",
        "  diagnostic_plots(df, 'col_rec')\n",
        "  # this applies logarithmic trasnformation\n",
        "  df['col_log'] = np.log(df[col]+1)\n",
        "  diagnostic_plots(df, 'col_log')\n"
      ],
      "metadata": {
        "id": "gz30IadYxNYU",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:26:44.411623Z",
          "iopub.execute_input": "2024-06-15T14:26:44.412716Z",
          "iopub.status.idle": "2024-06-15T14:26:44.420479Z",
          "shell.execute_reply.started": "2024-06-15T14:26:44.412672Z",
          "shell.execute_reply": "2024-06-15T14:26:44.419318Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# try normal distribution\n",
        "# Now we try to check the distribution of values present in different columns after application of various transformations\n",
        "\n",
        "lst = ['O2Sat', 'Temp', 'MAP', 'BUN', 'Creatinine', 'Glucose', 'WBC', 'Platelets' ]\n",
        "\n",
        "# Fill remaining NaN values with the mean of each column\n",
        "for col in lst:\n",
        "    if col in df_train_impute.columns:\n",
        "        df_train_impute[col].fillna(df_train_impute[col].mean(), inplace=True)\n",
        "\n",
        "for i in lst:\n",
        "  print(i)\n",
        "  try_gaussian(df_train_impute, i)"
      ],
      "metadata": {
        "id": "7xBHCsjFzp5g",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:26:51.202414Z",
          "iopub.execute_input": "2024-06-15T14:26:51.202843Z",
          "iopub.status.idle": "2024-06-15T14:28:33.992535Z",
          "shell.execute_reply.started": "2024-06-15T14:26:51.202808Z",
          "shell.execute_reply": "2024-06-15T14:28:33.991303Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# after application of the above code, some redundant columns got added to the dataframe, which are removed through this line of code\n",
        "\n",
        "df_train_impute = df_train_impute.drop(columns = ['col_yj','col_1.5','col_.5','col_rec','col_log'])"
      ],
      "metadata": {
        "id": "HGYde9LJI7iK",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:29:07.03878Z",
          "iopub.execute_input": "2024-06-15T14:29:07.039208Z",
          "iopub.status.idle": "2024-06-15T14:29:07.134739Z",
          "shell.execute_reply.started": "2024-06-15T14:29:07.039172Z",
          "shell.execute_reply": "2024-06-15T14:29:07.13361Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_impute.head()"
      ],
      "metadata": {
        "id": "xsqAmmk8BwY0",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:29:14.649167Z",
          "iopub.execute_input": "2024-06-15T14:29:14.649582Z",
          "iopub.status.idle": "2024-06-15T14:29:14.677555Z",
          "shell.execute_reply.started": "2024-06-15T14:29:14.649546Z",
          "shell.execute_reply": "2024-06-15T14:29:14.676318Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# by oserving different plots, it can be concluded that only log was somewhat effective and that too for MAP, BUN, Creatinine, Glucose, WBC & Plateletes\n",
        "# therefore applying log transformations on the above columns\n",
        "\n",
        "columns_normalized = ['MAP', 'BUN', 'Creatinine', 'Glucose', 'WBC', 'Platelets' ]\n",
        "for i in columns_normalized:\n",
        "  df_train_impute[i] = np.log(df_train_impute[i]+1)"
      ],
      "metadata": {
        "id": "vynAfGRT8ln-",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:29:27.619743Z",
          "iopub.execute_input": "2024-06-15T14:29:27.620955Z",
          "iopub.status.idle": "2024-06-15T14:29:27.687475Z",
          "shell.execute_reply.started": "2024-06-15T14:29:27.620911Z",
          "shell.execute_reply": "2024-06-15T14:29:27.68646Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_impute.head()"
      ],
      "metadata": {
        "id": "_q143WfoQ-wB",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:29:33.45722Z",
          "iopub.execute_input": "2024-06-15T14:29:33.457628Z",
          "iopub.status.idle": "2024-06-15T14:29:33.483864Z",
          "shell.execute_reply.started": "2024-06-15T14:29:33.457592Z",
          "shell.execute_reply": "2024-06-15T14:29:33.482581Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# standard normalization\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_train_impute[['HR', 'O2Sat', 'Temp', 'MAP', 'Resp', 'BUN', 'Chloride',\n",
        "       'Creatinine', 'Glucose', 'Hct', 'Hgb', 'WBC', 'Platelets']] = scaler.fit_transform(df_train_impute[['HR', 'O2Sat', 'Temp', 'MAP', 'Resp', 'BUN', 'Chloride',\n",
        "       'Creatinine', 'Glucose', 'Hct', 'Hgb', 'WBC', 'Platelets']])\n",
        "df_train_impute.head()"
      ],
      "metadata": {
        "id": "3pWbo_AdJnmC",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:29:56.687293Z",
          "iopub.execute_input": "2024-06-15T14:29:56.68771Z",
          "iopub.status.idle": "2024-06-15T14:29:57.05356Z",
          "shell.execute_reply.started": "2024-06-15T14:29:56.687674Z",
          "shell.execute_reply": "2024-06-15T14:29:57.052348Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_impute = df_train_impute.dropna()"
      ],
      "metadata": {
        "id": "_ew4pdZ_OrmJ",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:30:01.815892Z",
          "iopub.execute_input": "2024-06-15T14:30:01.816952Z",
          "iopub.status.idle": "2024-06-15T14:30:01.930897Z",
          "shell.execute_reply.started": "2024-06-15T14:30:01.816909Z",
          "shell.execute_reply": "2024-06-15T14:30:01.929562Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "null_values = df_train_impute.isnull().mean()*100\n",
        "null_values"
      ],
      "metadata": {
        "id": "NWCmJR4OP4oi",
        "execution": {
          "iopub.status.busy": "2024-06-15T14:30:08.992136Z",
          "iopub.execute_input": "2024-06-15T14:30:08.99256Z",
          "iopub.status.idle": "2024-06-15T14:30:09.031011Z",
          "shell.execute_reply.started": "2024-06-15T14:30:08.992523Z",
          "shell.execute_reply": "2024-06-15T14:30:09.029742Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building"
      ],
      "metadata": {
        "id": "NeHzPa69fxTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function will transform the dataframe (for final testing) in the compatible\n",
        "# format for the input of the model\n",
        "# this will make it easier to get the dataframe ready in a single go for prediction\n",
        "\n",
        "def get_data_ready(df):\n",
        "  columns_drop={'Unnamed: 0','SBP','DBP','EtCO2','BaseExcess', 'HCO3','pH','PaCO2','Alkalinephos', 'Calcium','Magnesium',\n",
        "  'Phosphate','Potassium','PTT','Fibrinogen','Unit1','Unit2'}\n",
        "  df = df.assign(Unit=df['Unit1'] + df['Unit2'])\n",
        "  # dropping columns based on redundancy\n",
        "  df = df.drop(columns=columns_drop)\n",
        "  grouped_by_patient = df.groupby('Patient_ID')\n",
        "  # imputing backfill and forward fill\n",
        "  df = grouped_by_patient.apply(lambda x: x.bfill().ffill())\n",
        "  # dropping all the columns with null values more than 25% and patient_id\n",
        "  null_col = ['TroponinI', 'Bilirubin_direct', 'AST', 'Bilirubin_total', 'Lactate', 'SaO2', 'FiO2','Unit', 'Patient_ID']\n",
        "  df = df.drop(columns=null_col)\n",
        "  # gaussian transformation\n",
        "  columns_normalized = ['MAP', 'BUN', 'Creatinine', 'Glucose', 'WBC', 'Platelets' ]\n",
        "  for i in columns_normalized:\n",
        "    df[i] = np.log(df[i]+1)\n",
        "  # normailizing\n",
        "  scaler = StandardScaler()\n",
        "  df[['HR', 'O2Sat', 'Temp', 'MAP', 'Resp', 'BUN', 'Chloride',\n",
        "       'Creatinine', 'Glucose', 'Hct', 'Hgb', 'WBC', 'Platelets']] = scaler.fit_transform(df[['HR', 'O2Sat', 'Temp', 'MAP', 'Resp', 'BUN', 'Chloride',\n",
        "       'Creatinine', 'Glucose', 'Hct', 'Hgb', 'WBC', 'Platelets']])\n",
        "  # onehot encoding the gender\n",
        "  one_hot = pd.get_dummies(df['Gender'])\n",
        "  df = df.join(one_hot)\n",
        "  df = df.drop('Gender', axis=1)\n",
        "  df = df.dropna()\n",
        "  df.columns = df.columns.astype(str) # Convert column names to strings\n",
        "  return df"
      ],
      "metadata": {
        "id": "UvhSkWj3fzSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this fucntion calculates different evaluation parameters of a model\n",
        "\n",
        "def evaluate_model(y_true,y_pred):\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  print(\"Precision:\", precision)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  print(\"Recall:\", recall)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  print(\"F1 Score:\", f1)\n",
        "  auc = roc_auc_score(y_true, y_pred)\n",
        "  print(\"AUC-ROC:\", auc)\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  print(\"Mean Absolute Error:\", mae)\n",
        "  rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "  print(\"Root Mean Squared Error:\", rmse)\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  sns.heatmap(cm, annot=True, fmt='d')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "vJ2QYkdGf1Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the distribution of data points between the two classes\n",
        "\n",
        "majority_class = df_train_impute[df_train_impute['SepsisLabel'] == 0]\n",
        "minority_class = df_train_impute[df_train_impute['SepsisLabel'] == 1]\n",
        "print('number of sepsis label 1 is {}'.format(len(minority_class)))\n",
        "print('while number of sepsis label 0 is {}'.format(len(majority_class)))"
      ],
      "metadata": {
        "id": "WCN8ik5mf3x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, this shows a clear imbalance between sepsis label 1 and label 0, to deal with this, we did undersampling."
      ],
      "metadata": {
        "id": "2k5gcP6zf_Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersampling\n",
        "\n",
        "majority_class_subset = majority_class.sample(n=2*len(minority_class))\n",
        "df_train_impute = pd.concat([majority_class_subset, minority_class])"
      ],
      "metadata": {
        "id": "UIO7k924f6b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix(df_train_impute)"
      ],
      "metadata": {
        "id": "Spgo_xgGgBXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "J2B6YyuigL-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split for the imputed output\n",
        "X = df_train_impute.drop('SepsisLabel', axis=1)\n",
        "y = df_train_impute['SepsisLabel']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "AoWEamSPgE8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random forest classifier\n",
        "# random forest classifier was tested on different hyper parameters and it gave the best results with number of estimators as 300\n",
        "\n",
        "# model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "# model = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "model = RandomForestClassifier(n_estimators=300, random_state=0)\n",
        "\n",
        "# Reset index and convert column names to strings\n",
        "X_train = X_train.reset_index().set_index('Patient_ID')\n",
        "X_test = X_test.reset_index().set_index('Patient_ID')\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "rcf_predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "S5rytq5IgQIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_test,rcf_predictions)"
      ],
      "metadata": {
        "id": "uWBj_1U3gnnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier\n"
      ],
      "metadata": {
        "id": "Pmj1PiTEg97J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NBC performed worse than random forest on each and every aspect of the evaluation metrics\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "nbc_predictions = model.predict(X_test)\n",
        "evaluate_model(y_test,nbc_predictions)"
      ],
      "metadata": {
        "id": "79w9B5CtgoM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Classifier\n"
      ],
      "metadata": {
        "id": "YqFKK5AJhGzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN was tested on different values of k\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# model = KNeighborsClassifier(n_neighbors=8)\n",
        "# model = KNeighborsClassifier(n_neighbors=5)\n",
        "model = KNeighborsClassifier(n_neighbors=10)\n",
        "model.fit(X_train, y_train)\n",
        "knn_predictions = model.predict(X_test)\n",
        "evaluate_model(y_test,knn_predictions)"
      ],
      "metadata": {
        "id": "G6xEuIJshFY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n"
      ],
      "metadata": {
        "id": "77PHLiTkhMyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "lr_predictions = model.predict(X_test)\n",
        "evaluate_model(y_test,lr_predictions)"
      ],
      "metadata": {
        "id": "8Z38_zqihLdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# XGBoost\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EySKKyjhGOMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost was run for different combinations of hyperparameters, but overall random forest classifier performed the best in terms of F1 score and other metrics\n",
        "\n",
        "import xgboost as xgb\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "param = {\n",
        "    'max_depth': 5,  # the maximum depth of each tree\n",
        "    'eta': 0.3,  # the training step for each iteration\n",
        "    'silent': 1,  # logging mode - quiet\n",
        "    'objective': 'binary:logistic'}  # error evaluation for binary classification\n",
        "num_round = 100\n",
        "bst = xgb.train(param, dtrain, num_round)\n",
        "xgb_predictions = bst.predict(dtest)\n",
        "prediction = []\n",
        "for i in xgb_predictions:\n",
        "  if i<0.5:\n",
        "    prediction.append(0)\n",
        "  else:\n",
        "    prediction.append(1)\n",
        "evaluate_model(y_test,prediction)"
      ],
      "metadata": {
        "id": "FJta4fLNGOib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on the data of the other hospital"
      ],
      "metadata": {
        "id": "Xy5LICGPGXBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing on the other hospital data\n",
        "\n",
        "df = get_data_ready(df_test)\n",
        "X = df.drop('SepsisLabel', axis=1)\n",
        "y = df['SepsisLabel']\n",
        "\n",
        "# Apply the same index resetting and setting as done for training data\n",
        "X = X.reset_index().set_index('Patient_ID')\n",
        "X.columns = X.columns.astype(str) # Ensure column names are strings\n",
        "\n",
        "rcf_predictions = model.predict(X)\n",
        "evaluate_model(y,rcf_predictions)"
      ],
      "metadata": {
        "id": "_dPbnGGEGYoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_impute.to_pickle('df_train_impute.pkl')"
      ],
      "metadata": {
        "id": "L4-1lXK3VVk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}